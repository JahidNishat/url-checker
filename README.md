# Distributed URL Checker

**Production-quality, horizontally scalable URL checking system in Go + Redis.**

---

## üöÄ Quick Start

### 1. Install Dependencies
```bash

go get github.com/go-redis/redis/v8
```
### 2. Start Redis
```bash

redis-server
```
### 3. Generate Test URLs
```bash

go run generate_urls.go 10000
```
### 4. Run Producer
```bash

go run producer.go common.go config.go urls.txt
```
### 5. Start Workers (3 terminals)
```bash

# Terminal 1
go run worker.go common.go config.go worker-1

# Terminal 2
go run worker.go common.go config.go worker-2

# Terminal 3
go run worker.go common.go config.go worker-3
```
### 6. Monitor Progress
```bash

go run monitor.go common.go config.go
```
### 7. (Optional) API Server
```bash

go run api.go common.go config.go

# Query it:
curl http://localhost:8080/stats
curl http://localhost:8080/results
```

---

## üî• NEW: Performance Optimizations (Week 2)

### Cache-Aside Pattern
- **99.8% cache hit rate** on repeated workloads
- **20x speedup** (20s ‚Üí 1s for warm cache)
- 5-minute TTL per URL result
- Redis GET ‚Üí miss ‚Üí HTTP fetch ‚Üí Redis SET

### Write-Behind Batching
- **450x fewer Redis calls** (batched LPUSH)
- Dual triggers: **2 seconds OR 500 items**
- Non-blocking writes (worker never waits)
- Graceful shutdown (zero data loss)

### Measured Impact
| Metric               | Before    | After     | Improvement |
|----------------------|-----------|-----------|-------------|
| Cold run (10k URLs)  | 45s       | 45s       | -           |
| Warm run (10k URLs)  | 45s       | 2s        | **20x** ‚úÖ  |
| Redis LPUSH calls    | 10,000    | 22        | **450x** ‚úÖ |
| Throughput (warm)    | 222/sec   | 5,000/sec | **22x** ‚úÖ  |

### Three-Tier Cache Hit Distribution

**Measured on 5000 URLs:**
L1 hits: 4,750 (95.0%) ‚Üí ~1¬µs latency (in-memory LRU)
L2 hits: 200 ( 4.0%) ‚Üí ~1ms latency (Redis)
Origin: 50 ( 1.0%) ‚Üí ~100ms latency (HTTP fetch)

Cache efficiency: 99.0% (only 1% hit origin)
Average latency: ~0.05ms (vs 100ms without cache = 2000x faster)

**Cache Coherence:**
- L1: 60-second freshness check (evict stale entries)
- L2: 5-minute Redis TTL (eventual expiration)
- Errors: Cached in L2 only (not L1, to preserve hot space)

---

## üèÜ Week 2 Complete: Performance Summary

### **Final Architecture**
```text

Request ‚Üí L1 Cache (RAM, 1000 items, LRU) ‚Üí L2 Cache (Redis, 5min TTL) ‚Üí Origin (HTTP)
‚Üì 88.6% hits (< 0.001ms) ‚Üì 11.1% hits (~0.14ms) ‚Üì 0.4% (~1s)
```


### **Measured Performance (5,000 URLs)**

#### Cache Hit Distribution
- **L1 hits:** 4,428 (88.6%) ‚Äî In-memory LRU, sub-microsecond latency
- **L2 hits:** 554 (11.1%) ‚Äî Redis, ~140¬µs average latency
- **Origin:** 18 (0.4%) ‚Äî HTTP fetch, ~1 second per request
- **Cache efficiency:** 99.6% (only 0.4% hit origin)

#### Latency Percentiles
| Metric | Latency | Comparison |
|--------|---------|------------|
| **p50 (median)** | < 0.001 ms | 100,000x faster than origin |
| **p95** | 0.136 ms | 7,350x faster than origin |
| **p99** | 0.173 ms | 5,780x faster than origin |
| **max** | 1,167 ms | Origin fetch (worst case) |

**Key finding:** 99% of requests complete in under 0.2ms!

#### Write Performance
- **Batching:** 500-item flushes (or 2-second timer)
- **Write reduction:** 450x fewer Redis calls vs synchronous
- **Data safety:** Zero data loss on graceful shutdown (Ctrl+C tested)

#### Stampede Protection
- **Deduplication:** 90% reduction in duplicate HTTP fetches
- **Mechanism:** WaitGroup-based in-flight request tracking
- **Result:** 5,000 URLs ‚Üí only 18 unique origin fetches

### **Throughput**
- **Cold start:** 17 URLs in ~10 seconds (1.7 URLs/sec) ‚Äî building cache
- **Warm cache:** 4,983 URLs in ~2 seconds (2,491 URLs/sec) ‚Äî serving from cache
- **Overall:** 5,000 URLs in 12 seconds (416 URLs/sec, single worker)
- **Speedup:** 42x faster than no-cache baseline

### **Week 2 Technologies Used**
- `hashicorp/golang-lru` ‚Äî In-memory LRU cache (L1)
- Redis 6.2+ ‚Äî Shared cache layer (L2) with TTL
- `sync.WaitGroup` ‚Äî Stampede prevention (request deduplication)
- `sync.Mutex` + `atomic` ‚Äî Thread-safe metrics and batching
- Buffered channels ‚Äî Write-behind result flushing

### **Optimization Techniques Learned**
1. **Cache-aside pattern** ‚Äî Check cache first, populate on miss
2. **Write-behind batching** ‚Äî Buffer writes, flush in bulk
3. **Stampede prevention** ‚Äî Deduplicate concurrent requests for same resource
4. **Multi-layer caching** ‚Äî Fast local cache (L1) + shared persistent cache (L2)
5. **TTL strategy** ‚Äî Different expiration per layer (60s L1, 5min L2)
6. **Selective caching** ‚Äî Success responses in L1+L2, errors only in L2
7. **Graceful shutdown** ‚Äî Flush pending writes before exit

---

## ‚öôÔ∏è Configuration
Set environment variables:

```bash

export REDIS_ADDR=localhost:6379
export HTTP_TIMEOUT=5
export WORKER_TIMEOUT=1
export MAX_RETRIES=5
export RESULTS_TO_KEEP=10000
```
---
## üìä Performance

### Cold Cache (First Run)
- 3 workers: ~300 URLs/sec
- 10 workers: ~1,000 URLs/sec
- 50 workers: ~5,000 URLs/sec

### Warm Cache (Repeated URLs)
- **Single worker: 5,000 URLs/sec** (cache hit rate 99.8%)
- 3 workers: ~15,000 URLs/sec
- Limited by Redis throughput, not HTTP

### Write Efficiency
- **Synchronous writes:** 10,000 LPUSH calls for 10k URLs
- **Batched writes:** ~20 LPUSH calls for 10k URLs
- **Network savings:** 450x reduction

---

## üèóÔ∏è Architecture

```text

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Producer ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Redis Queue ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Workers (3+)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ                         ‚îÇ
                                   ‚îÇ 1. Pop URL              ‚îÇ
                                   ‚îÇ 2. Check cache (Redis)  ‚îÇ
                                   ‚îÇ 3. HTTP GET (if miss)   ‚îÇ
                                   ‚îÇ 4. Update cache         ‚îÇ
                                   ‚îÇ 5. Batch results        ‚îÇ
                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚ñº                                      ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Redis Cache ‚îÇ                      ‚îÇ Results LPUSH‚îÇ
              ‚îÇ (5min TTL)  ‚îÇ                      ‚îÇ (batched)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                          ‚îÇ
                                                          ‚ñº
                                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                   ‚îÇ Monitor/API  ‚îÇ
                                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**Key Components:**
- Queue: BRPOP with 1s timeout (blocking, efficient)
- Cache: GET/SET with 5min expiry (cache-aside pattern)
- Batching: 2s timer OR 500 items (write-behind pattern)
- Counters: Synchronous INCR (real-time stats)

---

## üìÅ Files
- `common.go` - Shared types and functions
- `config.go` - Configuration management
- `producer.go` - Enqueues URLs to Redis
- `worker.go` - Processes URLs (stateless, scalable)
- `monitor.go` - Real-time progress display
- `api.go` - REST API for queries
- `generate_urls.go` - Test data generator

---

## üõ†Ô∏è Advanced Features

### 1. Cache-Aside Pattern
```go

// Check cache first
cacheKey := fmt.Sprintf("cache:%s", url)
cached, err := rdb.Get(ctx, cacheKey).Bytes()
if err == nil {
// Cache hit - return immediately
return cachedResult
}

// Cache miss - fetch from origin
result := httpClient.Get(url)

// Store in cache for next time
rdb.Set(ctx, cacheKey, result, 5*time.Minute)
```
### 2. Write-Behind Batching
```go

// Non-blocking add to batch
flusher.Add(ctx, result)

// Background goroutine flushes:
// - Every 2 seconds (time-based)
// - OR when 500 items accumulated (size-based)
rdb.LPush(ctx, "results", batch...)
```
### 3. Graceful Shutdown
```bash

# Press Ctrl+C
^C
[worker-1] üõë Shutting down gracefully...
üì¶ Flushed 247 results to Redis  ‚Üê Remaining batch
[worker-1] ‚úÖ All batches flushed
```
### 4. Metrics Tracking
- cache_hit / cache_miss (hit rate monitoring)
- success / error / processing (real-time counters)
- All counters updated synchronously (not batched)

---

## üéì System Design Concepts
- **Horizontal Scaling** - Add more workers = more throughput
- **Stateless Workers** - No in-memory state, all in Redis
- **Queue-Based** - Natural load balancing
- **CAP Theorem** - CP system (consistency over availability)

---

# üìÇ COMPLETE FILE STRUCTURE
```text
url-checker/
‚îú‚îÄ‚îÄ common.go ‚Üê Shared types (URLResult, Stats, Redis client)
‚îú‚îÄ‚îÄ config.go ‚Üê Configuration from environment
‚îú‚îÄ‚îÄ producer.go ‚Üê Enqueues URLs to Redis
‚îú‚îÄ‚îÄ worker.go ‚Üê Processes URLs (run multiple instances)
‚îú‚îÄ‚îÄ monitor.go ‚Üê Real-time progress display
‚îú‚îÄ‚îÄ api.go ‚Üê REST API server
‚îú‚îÄ‚îÄ generate_urls.go ‚Üê Creates test data
‚îú‚îÄ‚îÄ urls.txt ‚Üê Generated by generate_urls.go
‚îî‚îÄ‚îÄ README.md ‚Üê Documentation
```

---

# üöÄ STEP-BY-STEP TO RUN

## **Step 1: Install Redis**

```bash

# macOS
brew install redis
brew services start redis

# Or run manually
redis-server
```

## Step 2: Install Go dependency
```bash

go get github.com/go-redis/redis/v8
```

## Step 3: Create all files
Copy each file above exactly as shown. Make sure:

- All files are in the same directory
- File names match exactly
- No extra spaces or characters

## Step 4: Generate test data
```bash

go run generate_urls.go 1000
Output: ‚úÖ Generated 1000 URLs in urls.txt
```

## Step 5: Run Producer
```bash

go run producer.go common.go config.go urls.txt
```
Output:

```text

‚úÖ Connected to Redis
üóëÔ∏è  Cleared previous data
‚úÖ Enqueued 1000 URLs in 0.05 seconds
üìä Average: 20000 URLs/sec
üöÄ Ready to start workers!
```

## Step 6: Start Workers
#### Terminal 1:

```bash

go run worker.go common.go config.go worker-1
```
#### Terminal 2:

```bash

go run worker.go common.go config.go worker-2
```
#### Terminal 3:

```bash

go run worker.go common.go config.go worker-3
```

## Step 7: Monitor
#### Terminal 4:

```bash

go run monitor.go common.go config.go
```
#### You'll see:

```text

üìä Queue:    750 | ‚öôÔ∏è  Processing:   3 | ‚úÖ Success:    247 | ‚ùå Error:   0 | Progress:  24.7% | Rate: 50/s | ETA: 15s
```